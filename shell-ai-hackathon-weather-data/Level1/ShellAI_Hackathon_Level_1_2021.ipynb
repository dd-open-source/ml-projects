{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ShellAI-Hackathon-Level-1-2021.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1hubEjkOHah"
      },
      "source": [
        "We stored training and testing datasets in google drive so need to mount drive inorder to access the files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq5QsFbYbxT7",
        "outputId": "34117e0e-8962-43a5-b4f7-0fa2226ca395"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRNAHcg5OcNR"
      },
      "source": [
        "Python modules used for the hackathon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSNgVgg8Pis0"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "\n",
        "import IPython\n",
        "import IPython.display\n",
        "from IPython.display import clear_output\n",
        "from google.colab import files\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gjwwobkPLlW"
      },
      "source": [
        "We converted all individual test scenario files into single test file for easy usage. Following script need to be executed only one time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zPhbcl0cuH0"
      },
      "source": [
        "# test_file_template = '/content/drive/MyDrive/ML_Projects/shell-ai-data/test/{}/weather_data.csv'\n",
        "# test_dfs=[]\n",
        "# for i in range(1, 301):\n",
        "#     test_file = test_file_template.format(i)\n",
        "#     df = pd.read_csv(test_file)\n",
        "#     test_dfs.append(df)\n",
        "\n",
        "# for idx, test_df in enumerate(test_dfs):\n",
        "#   test_df['scenario_set'] = idx+1\n",
        "\n",
        "# test_df = pd.concat(test_dfs, axis=0).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiYDsjT9PulL"
      },
      "source": [
        "Read train and test datasets and convert to pandas Dataframes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0e-0vkyP5gZ"
      },
      "source": [
        "training_path = '/content/drive/MyDrive/ML_Projects/shell-ai-data/train.csv'\n",
        "train_df = pd.read_csv(training_path)\n",
        "testing_path = '/content/drive/MyDrive/ML_Projects/shell-ai-data/test.csv'\n",
        "test_df = pd.read_csv(testing_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQsqhxOrQZxf"
      },
      "source": [
        "**perform_preprocessing** does the following feature Engineering steps: \n",
        "* Converted wind speed into wind velocity vector (velocity along x and y axes)\n",
        "* Converted Azimuth angle in agree to sin and cos values as they better captures the cyclic nature.\n",
        "* Used Forward-Fill (ffill method in pandas) to fill the missing values in target column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmzRw_peQABg"
      },
      "source": [
        "def perform_preprocessing(df):\n",
        "  peak_wind_speed_col = 'Peak Wind Speed @ 6ft [m/s]'\n",
        "  avg_wind_dir_col = 'Avg Wind Direction @ 6ft [deg from N]'\n",
        "  azimuth_angle_col = 'Azimuth Angle [degrees]'\n",
        "\n",
        "  wind_speed = df.pop(peak_wind_speed_col)\n",
        "  bad_wv = wind_speed < 0\n",
        "  wind_speed[bad_wv] = 0.0\n",
        "  wind_angle = df.pop(avg_wind_dir_col)*np.pi / 180\n",
        "\n",
        "  df['wind_x'] = wind_speed*np.cos(wind_angle)\n",
        "  df['wind_y'] = wind_speed*np.sin(wind_angle)\n",
        "\n",
        "  azimuth_angle = df.pop(azimuth_angle_col) * np.pi/180\n",
        "  df['azimuth_sin'] = np.sin(azimuth_angle)\n",
        "  df['azimuth_cos'] = np.cos(azimuth_angle)\n",
        "\n",
        "  total_cloud_cover = 'Total Cloud Cover [%]'\n",
        "  df.loc[df[total_cloud_cover]<0, total_cloud_cover] = np.nan\n",
        "  df = df.fillna(method='ffill')\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8Vbvs4fR0rc"
      },
      "source": [
        "**prepare_timeseries_records** function takes time series dataset coverts to multi-step regression problem.\n",
        "\n",
        "Example: This function converts following dataframe\n",
        "\n",
        "Time | Feature1 | Feature2\n",
        "-----|----------|--------\n",
        "T1   | 1.1 | 2.1 \n",
        "T2   | 1.2 | 2.2\n",
        "T3   | 1.3 | 2.3 \n",
        "\n",
        "to\n",
        "\n",
        "Feature1-1 | Feature2-1 | Feature1-2 | Feature2-2 \n",
        "-----------|------------|------------|-----------\n",
        "1.1        | 1.2        | 2.1        | 2.2 \n",
        "1.2        | 1.3        | 2.2        | 2.3 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxHBUnLsQDlj"
      },
      "source": [
        "def prepare_timeseries_records(df, sampling_rate, stride, steps, start_offset=0, end_offset=None):\n",
        "  timeseries_dfs = [[]]*steps\n",
        "  for i in range(steps):\n",
        "    timeseries_dfs[i] = df[i*sampling_rate+start_offset:end_offset:stride].reset_index(drop=True)\n",
        "    timeseries_dfs[i].columns = [\"{}_{}\".format(c, i+1) for c in timeseries_dfs[i].columns]\n",
        "  return pd.concat(timeseries_dfs, axis=1).dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBKLqSK6VrIt"
      },
      "source": [
        "**prepare_dfs** groups dataset by date/scenario and creates train and test datasets using above functions.\n",
        "* We are grouping dataset by date so that data is not combined across different dates as we need to predict using data within a day in the test set. \n",
        "* We are also ignoring data with missing values from at start and end of the day. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qu_ecKQQGxj"
      },
      "source": [
        "def prepare_dfs(full_df, groupby, columns, sampling_rate=10, stride=5, input_steps=12):\n",
        "  x_train_dfs = []\n",
        "  y_train_dfs = []\n",
        "  for _, day_df in full_df.groupby(groupby):\n",
        "    day_df = day_df[columns].reset_index(drop=True)\n",
        "    valid_indices = day_df.index[day_df['Total Cloud Cover [%]'] >= 0]\n",
        "    min_valid = min(valid_indices)\n",
        "    max_valid = max(valid_indices)  \n",
        "\n",
        "    df = day_df.iloc[min_valid:max_valid, :].reset_index(drop=True)\n",
        "    df = perform_preprocessing(df)\n",
        "    x_train_dfs.append(prepare_timeseries_records(df, sampling_rate, stride, input_steps, end_offset=-120))\n",
        "    y_train_dfs.append(prepare_timeseries_records(df[['Total Cloud Cover [%]']], sampling_rate, stride, 120//sampling_rate, start_offset=input_steps*sampling_rate))\n",
        "  \n",
        "  return pd.concat(x_train_dfs, axis=0).reset_index(drop=True), pd.concat(y_train_dfs, axis=0).reset_index(drop=True).iloc[:, ::(30//sampling_rate)].reset_index(drop=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwGfGccKYFjE"
      },
      "source": [
        "**prepare_df_for_predictions** makes test dataset to predict and submit them for evaluation in the portal. We need to this function with same parameters used for training set and it should have same set of features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYnvUhSnIyRK"
      },
      "source": [
        "def prepare_df_for_predictions(full_df, groupby, columns, sampling_rate=10, input_steps=12):\n",
        "  x_train_dfs = []\n",
        "  for _, day_df in full_df.groupby(groupby):\n",
        "    df = day_df[columns].reset_index(drop=True)\n",
        "    df = perform_preprocessing(df)\n",
        "    x_train_dfs.append(prepare_timeseries_records(df, sampling_rate, 1, input_steps, start_offset=(360-sampling_rate*(input_steps-1))))\n",
        "  \n",
        "  return pd.concat(x_train_dfs, axis=0).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-OVdWashOS9"
      },
      "source": [
        "We are constructing training set using both **train_df** and **test_df** as we are using 2 hours data to predict next 2 hours. \n",
        "\n",
        "We are reserving a small portion from time series dataset created using test_df as it is the close represention of final dataset to be used for predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo5l-zfs-bv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b163f3cb-f014-41b9-d05e-5ac9b14bf6a8"
      },
      "source": [
        "SAMPLING_RATE=5\n",
        "INPUT_STEPS=24\n",
        "columns = ['Global CMP22 (vent/cor) [W/m^2]',\n",
        "       'Direct sNIP [W/m^2]', 'Azimuth Angle [degrees]',\n",
        "       'Tower Dry Bulb Temp [deg C]', 'Tower Wet Bulb Temp [deg C]',\n",
        "       'Tower Dew Point Temp [deg C]', 'Tower RH [%]', 'Total Cloud Cover [%]',\n",
        "       'Peak Wind Speed @ 6ft [m/s]', 'Avg Wind Direction @ 6ft [deg from N]',\n",
        "       'Station Pressure [mBar]', 'Precipitation (Accumulated) [mm]',\n",
        "       'Snow Depth [cm]', 'Moisture', 'Albedo (CMP11)']\n",
        "X,Y = prepare_dfs(train_df, 'DATE (MM/DD)', columns, sampling_rate=SAMPLING_RATE,stride=1, input_steps=INPUT_STEPS)\n",
        "X_test_train, Y_test_train = prepare_dfs(test_df, 'scenario_set', columns, sampling_rate=SAMPLING_RATE,stride=1, input_steps=INPUT_STEPS)\n",
        "X_train1, X_test, Y_train1, Y_test = train_test_split(X_test_train, Y_test_train, test_size=0.2) \n",
        "X_pred = prepare_df_for_predictions(test_df, 'scenario_set', columns, sampling_rate=SAMPLING_RATE, input_steps=INPUT_STEPS)\n",
        "X_full = pd.concat([X, X_train1], axis=0).reset_index(drop=True) \n",
        "Y_full = pd.concat([Y, Y_train1], axis=0).reset_index(drop=True) \n",
        "print(X_full.shape, Y_full.shape, X_test.shape, Y_test.shape, X_pred.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(195678, 384) (195678, 4) (7415, 384) (7415, 4) (300, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wcC9x_Jahiw"
      },
      "source": [
        "**Scaling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siJctCDdRN_t"
      },
      "source": [
        "train_mean = X_full.mean()\n",
        "train_std = X_full.std()\n",
        "\n",
        "X_train = (X_full - train_mean) / train_std\n",
        "X_test = (X_test - train_mean) / train_std\n",
        "Y_train = Y_full\n",
        "X_pred = (X_pred - train_mean) / train_std\n",
        "train_indexs = list(range(X_train.shape[0]))\n",
        "np.random.shuffle(train_indexs)\n",
        "X_train = X_train.iloc[train_indexs]\n",
        "Y_train = Y_train.iloc[train_indexs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmFmioPeLR2c"
      },
      "source": [
        "**Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbQUQBOJIPes"
      },
      "source": [
        "models_location = '/content/drive/MyDrive/ML_Projects/models/ShellAI/'\n",
        "MAX_EPOCHS = 5\n",
        "\n",
        "def compile_and_fit(model, x_train, y_train, model_path, epochs=5, patience=3):\n",
        "\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                    patience=patience,\n",
        "                                                    mode='min')\n",
        "\n",
        "  model.compile(loss='mean_absolute_error',\n",
        "                optimizer='adam')\n",
        "  \n",
        "  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=model_path,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True)\n",
        "\n",
        "  history = model.fit(x=x_train.values, y=y_train.values, epochs=epochs,\n",
        "                      validation_split=0.1,\n",
        "                      callbacks=[early_stopping, model_checkpoint_callback])\n",
        "  return history\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csnl86VHbU3X"
      },
      "source": [
        "We used combination of LSTM (long short term memory) and dense layers to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOneczNsLdJB",
        "outputId": "2a2ea8e4-ae5d-43b1-9b78-7ec4f71491ad"
      },
      "source": [
        "def create_model():\n",
        "    return tf.keras.Sequential([\n",
        "      tf.keras.layers.Reshape((INPUT_STEPS,-1), input_shape=(16*INPUT_STEPS,)),\n",
        "      # LSTM expects input shape (batch size, time steps, features)\n",
        "      tf.keras.layers.LSTM(32, return_sequences=False),\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      tf.keras.layers.Dense(4) # Output layer\n",
        "   ])\n",
        "    \n",
        "model = create_model()\n",
        "print(model.summary())\n",
        "\n",
        "model_name = \"third\"\n",
        "model_path = os.path.join(models_location, model_name, \"checkpoint\")\n",
        "history = compile_and_fit(model, X_train, Y_train, model_path, epochs=100, patience=3)\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_1 (Reshape)          (None, 24, 16)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 32)                6272      \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 512)               16896     \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 4)                 2052      \n",
            "=================================================================\n",
            "Total params: 2,651,780\n",
            "Trainable params: 2,651,780\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ni4obNQnW90"
      },
      "source": [
        "**Prediction**\n",
        "* Predicts target values and generates final submission file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAb5z_-AnsaV"
      },
      "source": [
        "min_loss_index = np.argmin(history.history['val_loss'])\n",
        "print(\"train_loss=\", history.history['loss'][min_loss_index],\"val_loss=\", history.history['val_loss'][min_loss_index])\n",
        "best_model = create_model()\n",
        "best_model.load_weights(tf.train.latest_checkpoint(os.path.join(models_location, model_name)))\n",
        "best_model.compile(loss='mean_absolute_error', optimizer='adam')\n",
        "print(\"test_loss=\", best_model.evaluate(X_test, Y_test, verbose=0))\n",
        "predictions = best_model.predict(X_pred.values)\n",
        "pred_df = pd.DataFrame(predictions, columns=['30_min_horizon', '60_min_horizon', '90_min_horizon', '120_min_horizon'])\n",
        "pred_df['scenario_set'] = [i+1 for i in range( pred_df.shape[0])]\n",
        "pred_df.set_index('scenario_set', inplace=True)\n",
        "pred_location = '/content/drive/MyDrive/ML_Projects/shell-ai-data/submission-{}.csv'.format(datetime.datetime.now().isoformat())\n",
        "pred_df.to_csv(pred_location)\n",
        "files.download(pred_location)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-gi9UdQ-KnN"
      },
      "source": [
        "# last_best_test_loss=3.4990718364715576\n",
        "# val_loss=3.408386468887329"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}